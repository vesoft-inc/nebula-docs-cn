# 运行配置要求

## 生产环境

### 生产环境部署方式

* 3 个元数据服务进程 `metad`
* 至少 3 个存储服务进程 `storaged`
* 至少 3 个查询引擎服务进程 `graphd`

以上进程都无需独占机器。例如一个由 5 台机器组成的集群：A、B、C、D、E，可以如下部署：

* A：metad, storaged, graphd
* B：metad, storaged, graphd
* C：metad, storaged, graphd
* D：storaged, graphd
* E：storaged, graphd

> 同一个集群不要跨机房部署。
> metad 每个进程都会创建一份元数据的存储副本，因此通常只需 3 个进程。storaged 进程数量不影响图空间数据的副本数量。

### 服务器配置要求(标准配置)

以 AWS EC2 c5d.12xlarge 为例：

* 处理器：48 core
* 内存：96 GB
* 存储：2 * 900 GB, NVMe SSD
* Linux 内核：3.9 或更高版本，通过命令 `uname -r` 查看
* glibc：2.12 或更高版本，通过命令 `ldd --version` 查看

操作系统配置见[这里](./7.kernel-config.md)。

## 测试环境

* 1 个元数据服务进程 `metad`
* 至少 1 个存储服务进程 `storaged`
* 至少 1 个查询引擎服务进程 `graphd`

例如一个有 3 台机器的集群：A、B、C 可以如下部署：

* A：metad、storaged、graphd
* B：storaged、graphd
* C：storaged、graphd

### 服务器配置要求(最低配置)

以 AWS EC2 c5d.xlarge 为例：

* 处理器：4 core
* 内存：8 GB
* 存储：100 GB, SSD

## 资源估算（3副本标准配置）

* 存储空间（全集群）：点和边数量 * 平均属性的字节数 * 6
* 内存（全集群）：点边数量 * 15 字节 + RocksDB 实例数量 * (write_buffer_size * max_write_buffer_number) + rocksdb_block_cache * storaged 进程数量。

  - 其中 `etc/nebula-storaged.conf` 文件中 `--data_path` 项中的每个目录对应一个 RocksDB 实例。
  - storaged 进程数量通常为机器的数量。

* 图空间 partition 数量：全集群的所有硬盘数量 * （2 至 10 —— 硬盘越好该值越大）
* 内存和硬盘另预留 20% buffer。

> 如果全集群的机器内存不够达到上述标准：设置 `--enable_partitioned_index_filter=true` ，可以减少 （`bloomfilter` 的）内存占用，相应会有读性能降低（各场景不同）。
> 存储默认压缩算法为 snappy, 可以改为 Lz4 以提高压缩比。

<!--
## 关于机械硬盘和千兆网络(不支持)

1. 设计和默认参数针对的硬件设备是 NVMe SSD 和万兆网。

2. **HDD 无法满足软件运行要求，实践中无法正常运行**。


如果你想测试，以下是一些需调整的参数：

* etc/nebula-storage.conf：
  * --raft_rpc_timeout_ms= 5000 至 10000
  * --rocksdb_batch_size= 4096 至 16384
  * --heartbeat_interval_secs = 30 至 60
  * --raft_heartbeat_interval_secs = 30 至 60
* etc/nebula-meta.conf：
  * --heartbeat_interval_secs 与 etc/nebula-storage.conf 该项相同
* Spark Writer:

```text
rate: {
      timeout: 5000 至 10000
    }
```

* go-importer:
  * batchSize: 10 至 50
  * concurrency: 1 至 10
  * channelBufferSize：100 至 500
* partition 值为全集群硬盘数量 2 倍
-->