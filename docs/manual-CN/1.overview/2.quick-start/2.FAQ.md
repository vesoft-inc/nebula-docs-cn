# 常见问题

本文档列出了 **Nebula Graph** 常见问题。如果您没有在文档中找到需要的信息，请尝试在 Nebula Graph [官方论坛](https://discuss.nebula-graph.com.cn/)中的[用户问答](https://discuss.nebula-graph.com.cn/c/users/5)分类下进行搜索。

## General Information

General Information 部分列出了关于 **Nebula Graph** 的概念性问题。

### 查询返回时间解释

```ngql
nebula> GO FROM 101 OVER follow;
===============
| follow._dst |
===============
| 100         |
---------------
| 102         |
---------------
| 125         |
---------------
Got 3 rows (Time spent: 7431/10406 us)
```

以上述查询为例，Time spent 中前一个数字 `7431` 为数据库本身所花费的时间，即 query engine 从 console 收到这条查询语句，到存储拿到数据，并进行一系列计算所花的时间；后一个数字 `10406` 是从客户端角度看花费的时间，即 console 从发送请求，到收到响应，并将结果输出到屏幕的时间。

## Trouble Shooting

Trouble Shooting 部分列出了 **Nebula Graph** 操作中的常见错误。

### 服务器参数配置

在 Nebula console 中运行

```ngql
nebula> SHOW CONFIGS;
```

详细参考[运行配置要求](../../3.build-develop-and-administration/3.configurations/0.system-requirement.md)及 [CONFIGS 语法](../../3.build-develop-and-administration/3.configurations/2.configs-syntax.md)。

### 配置文件

配置文件默认在 `/usr/local/nebula/etc/` 下。

配置参考[这里](../../3.build-develop-and-administration/3.configurations/0.system-requirement.md)及 [CONFIGS 语法](../../3.build-develop-and-administration/3.configurations/2.configs-syntax.md)。

### partition 分布不均

参考[这里](../../3.build-develop-and-administration/5.storage-service-administration/storage-balance.md)。

### 日志和更改日志级别

日志文件默认在 `/usr/local/nebula/logs/` 下。

参见 [graphd 日志](../../3.build-develop-and-administration/3.configurations/4.graph-config.md) 和 [storaged 日志](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)。

### 使用多块硬盘

修改 `/usr/local/nebula/etc/nebula-storage.conf`。例如

```text
--data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/
```

多块硬盘时可以逗号分隔多个目录，每个目录对应一个 RocksDB 实例，以有更好的并发能力。参考[这里](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)。

### 进程异常 crash

1. 检查硬盘空间 `df -h`

    没有磁盘空间，导致服务写文件失败，服务 crash，通过以上命令查看当前磁盘的使用情况，服务配置的 `--data_path` 目录是否是满的目录。

2. 检查内存是否足够 `free -h`

    服务使用过多内存，被系统杀掉，通过 `dmesg` 查看是否有 OOM 的记录，记录是否有 nebula 关键字。

3. 检查日志

### 使用 Docker 启动后，执行命令时报错

可能的原因是 Docker 的 IP 地址和默认配置中的监听地址不一致（默认是 172.17.0.2），因此这里需要修改默认配置中的监听地址。

1. 首先在容器中执行 `ifconfig` 命令，查看您的容器地址，这里假设您的容器地址是 172.17.0.3，那么就意味着您需要修改默认配置的 IP 地址。
2. 然后进入配置目录 `cd /usr/local/nebula/etc`，查找所有 IP 地址配置的位置（ `grep "172.17.0.2" . -r`）。
3. 修改上一步查到的所有 IP 地址为您的容器地址（172.17.0.3）。
4. 最后重新启动所有服务（`/usr/local/nebula/scripts/nebula.service restart all`）。

### 单机先后加入两个不同集群

同一台主机先后用于单机测试和集群测试，storaged 服务无法正常启动（终端上显示的 storaged 服务的监听端口为红色）。查看 storaged 服务的日志（`/usr/local/nebula/nebula-storaged.ERROR`），若发现 "wrong cluster" 的报错信息，则可能的出错原因是单机测试和集群测试时的 Nebula Graph 生成的 cluster id 不一致，需要删除 Nebula Graph 安装目录（`/usr/local/nebula`）下的 cluster.id 文件和 data 目录，然后重启服务。

### 连接失败

```txt
E1121 04:49:34.563858   256 GraphClient.cpp:54] Thrift rpc call failed: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused
```

检查服务是否存在

```bash
$ /usr/local/nebula/scripts/nebula.service status all
或者
nebula> SHOW HOSTS;
```

### Could not create logging file:... Too many open files

1. 检查硬盘空间 `df -h`
2. 检查日志目录 `/usr/local/nebula/logs/`
3. 修改允许打开的最大文件数 `ulimit -n 65536`

### 如何查看 Nebula Graph 版本信息

使用 `curl http://ip:port/status` 命令获取 git_info_sha、binary 包的 commitID。

### 修改配置文件不生效

**Nebula Graph** 使用如下两种方式获取配置：

1. 从配置文件中（需要修改配置文件并重启服务）；
2. 从 Meta 服务中。通过 CLI 设置，并持久化保存在 Meta 服务中，详情参考[这里](../../3.build-develop-and-administration/3.configurations/1.config-persistency-and-priority.md)；

修改了配置文件不生效，是因为默认情况下，**Nebula Graph** 的配置参数管理采用第二种方式 （Meta），如果希望采用第一种方式，需要在 `/usr/local/nebula/etc/` 配置文件 `metad.conf`、 `storaged.conf`、`graphd.conf` 中分别添加 `--local_config=true` 选项。

### 修改  RocksDB block cache

更改 storage 的配置文件 `storaged.conf`（默认路径为 `/usr/local/nebula/etc/`）并重启，例如：

```bash
# Change rocksdb_block_cache to 1024 MB
--rocksdb_block_cache = 1024
# Stop storaged and restart
/usr/local/nebula/scripts/nebula.service stop storaged
/usr/local/nebula/scripts/nebula.service start storaged
```

参见[这里](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)

### 使用 CentOS 6.5 Nebula 服务失败

在 CentOS 6.5 部署 Nebula Graph 失败，报错信息如下：

```bash
# storage 日志
Heartbeat failed, status:RPC failure in MetaClient: N6apache6thrift9transport19TTransportExceptionE: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused

# meta 日志
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
E0415 22:32:38.944437 15532 AsyncServerSocket.cpp:762] failed to set SO_REUSEPORT on async server socket Protocol not available
E0415 22:32:38.945001 15510 ThriftServer.cpp:440] Got an exception while setting up the server: 92failed to bind to async server socket: [::]:0: Protocol not available
E0415 22:32:38.945057 15510 RaftexService.cpp:90] Setup the Raftex Service failed, error: 92failed to bind to async server socket: [::]:0: Protocol not available
E0415 22:32:38.949586 15463 NebulaStore.cpp:47] Start the raft service failed
E0415 22:32:38.949597 15463 MetaDaemon.cpp:88] Nebula store init failed
E0415 22:32:38.949796 15463 MetaDaemon.cpp:215] Init kv failed!
```

此时服务状态为：

```bash
[root@redhat6 scripts]# ./nebula.service status  all
[WARN] The maximum files allowed to open might be too few: 1024
[INFO] nebula-metad: Exited
[INFO] nebula-graphd: Exited
[INFO] nebula-storaged: Running as 15547, Listening on 44500
```

出错原因：CentOS 6.5 系统内核版本为 2.6.32，`SO_REUSEPORT` 仅支持 `Linux 3.9` 及以上版本。

将系统升级到 CentOS 7.5 问题可自行解决。

### max_edge_returned_per_vertex 和 WHERE 条件执行的优先顺序

如果已设置 max_edge_returned_per_vertex=10，使用 WHERE 进行过滤时，实际边数量大于 10，此时返回 10 条边，还是所有边？

**Nebula Graph** 先进行 WHERE 条件过滤，如果实际边数量少于 10， 则返回实际边数量。反之，则根据 max_edge_returned_per_vertex 限制返回 10 条边。

### 使用 `FETCH` 返回数据时，有时可以返回数据，有时返回为空

请检查是否在同一个节点启动了两个 storage，并且这两个 storage 配置的端口号相同。假如存在以上情况，请修改其中一个 storage 端口号，然后重新导入数据。

### sst 文件是否支持搬迁

storaged 所使用的 sst 文件和 集群图空间是绑定的，不支持直接拷贝给一个新的图空间或者集群使用。
