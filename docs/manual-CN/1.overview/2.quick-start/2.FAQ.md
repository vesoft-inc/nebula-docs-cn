# 常见问题

本文档列出了 **Nebula Graph** v1.1 常见问题。如果您没有在文档中找到需要的信息，请尝试在 Nebula Graph [官方论坛](https://discuss.nebula-graph.com.cn/)中的[用户问答](https://discuss.nebula-graph.com.cn/c/users/5)分类下进行搜索。

## Trouble Shooting

Trouble Shooting 部分列出了 **Nebula Graph** 操作中的疑问。

### 查询返回时间解释

```ngql
nebula> GO FROM 101 OVER follow;
===============
| follow._dst |
===============
| 100         |
---------------
| 102         |
---------------
| 125         |
---------------
Got 3 rows (Time spent: 7431/10406 us)
```

以上述查询为例，Time spent 中前一个数字 `7431` 为数据库本身所花费的时间，即 query engine 从 console 收到这条查询语句，到存储拿到数据，并进行一系列计算所花的时间；后一个数字 `10406` 是从客户端角度看花费的时间，即 console 从发送请求，到收到响应，并将结果输出到屏幕的时间。

### 软硬件要求

详细参考[运行配置要求](../../3.build-develop-and-administration/3.configurations/0.system-requirement.md)

### 参数配置

Nebula Graph 的配置机制较为复杂，简单的说有两种方式：读取本地配置文件，或者通过命令行控制。为避免错误，**两者只可选其一，不要混合使用**。

#### 本地配置文件

配置文件默认在 `/usr/local/nebula/etc/` 下。

在三个配置文件中都加上一行 `--local-config=true`，这样程序启动时将只依赖于配置文件。

此外 RocksDB 的启动日志在 `/usr/local/nebula/data/storage/nebula/{space_id}/data/LOG`

#### 命令行控制

例如，在 Nebula console 中运行

```ngql
nebula> SHOW CONFIGS;
```

> 命令行控制较为繁杂，容易有疏漏。推荐新用户从本地配置文件开始。

详细操作见：配置参考[这里](../../3.build-develop-and-administration/3.configurations/0.system-requirement.md)及 [CONFIGS 语法](../../3.build-develop-and-administration/3.configurations/2.configs-syntax.md)。

### 主机先后加入过两个不同集群 (wrong cluster)

同一台主机先后用于单机测试和集群测试，storaged 服务无法正常启动（终端上显示的 storaged 服务的监听端口为红色）。查看 storaged 服务的日志（`/usr/local/nebula/nebula-storaged.ERROR`），若发现 "wrong cluster" 的报错信息，则可能的出错原因是单机测试和集群测试时的 Nebula Graph 生成的 cluster id 不一致，需要删除 Nebula Graph 安装目录（`/usr/local/nebula`）下的 `cluster.id` 文件和 `data` 目录，然后重启服务。

### 进程 crash

1. 检查硬盘空间 `df -h`

    没有磁盘空间，导致服务写文件失败，服务 crash，通过以上命令查看当前磁盘的使用情况，服务配置的 `--data_path` 目录是否是满的目录。

2. 检查内存是否足够 `free -h`

    使用过多内存，被系统杀掉，通过 `dmesg` 查看是否有 OOM 的记录，记录是否有 nebula 关键字。

3. 检查日志，在`/usr/local/nebula/logs/`。

### partition 分布不均

partition 分布不均会导致部分主机压力过大，参考[这里](../../3.build-develop-and-administration/5.storage-service-administration/storage-balance.md)。

### 发生 storaged OOM

1. 检查资源规划是否足够，详细参考[运行配置要求](../../3.build-develop-and-administration/3.configurations/0.system-requirement.md)
1. 在`/usr/local/nebula/etc/nebula-storaged.conf` 中设置 `--enable_partitioned_index_filter=true` 和 `--local-config=true`
1. 检查 partition 分布。
1. 设置读取[截断参数](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)。只返回部分数据。

### 发生 graphd OOM

通常情况为单个查询过大。由于 Nebula Graph 的设计是一个查询只会由一个 graphd 处理。当发生大查询（例如很深的路径探索，或者返回极大量的数据）时，graphd 会发生 OOM。

### 什么是 compact

compact 的主要目的是通过重新组织硬盘上的文件结构，使得从“写优先”的文件格式，变成“读优先”的文件格式。具体操作见[这里](../../3.build-develop-and-administration/5.storage-service-administration/compact.md)

### 日志和更改日志级别

日志文件默认在 `/usr/local/nebula/logs/` 下。一般设置 `v=0`, 调试时可以设置为 `v=4`.

参见 [graphd 日志](../../3.build-develop-and-administration/3.configurations/4.graph-config.md) 和 [storaged 日志](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)。

### 使用多块硬盘

修改 `/usr/local/nebula/etc/nebula-storage.conf`。例如

```text
--data_path=/disk1/storage/,/disk2/storage/,/disk3/storage/
```

多块硬盘时可以逗号分隔多个目录，每个目录对应一个 RocksDB 实例，以有更好的并发能力。参考[这里](../../3.build-develop-and-administration/3.configurations/5.storage-config.md)。

### 连接失败，报错 Connection refused

```txt
E1121 04:49:34.563858   256 GraphClient.cpp:54] Thrift rpc call failed: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused
```

1. 检查服务是否存在

```bash
$ /usr/local/nebula/scripts/nebula.service status all
```

或者

```ngql
nebula> SHOW HOSTS;
```

2. 请确保以下端口可用，且未被防火墙阻止。您可以在 `/usr/local/nebula/etc/` 的配置文件中找到相关的端口信息。

服务名称|端口号
---|---
Meta 服务|45500, 11000 - 11002
Storage 服务|44500, 12000 - 12002
Graph 服务|3699, 13000 - 13002

此外，如果您更改了端口的默认值，请重新启动服务。

运行以下命令查看端口是否可用：

```bash
$ telnet $ip $port
```

确保集群中所有主机的端口均可用。

### Could not create logging file:... Too many open files

1. 检查硬盘空间 `df -h`
1. 修改允许打开的最大文件数 `ulimit -n 100000`

### 如何查看 Nebula Graph 版本信息

使用 `curl http://ip:port/status` 命令获取 git_info_sha、binary 包的 commitID。

### 使用 CentOS 6.5 Nebula 服务失败

在 CentOS 6.5 部署 Nebula Graph 失败，报错信息如下：

```bash
# storage 日志
Heartbeat failed, status:RPC failure in MetaClient: N6apache6thrift9transport19TTransportExceptionE: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused

# meta 日志
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
E0415 22:32:38.944437 15532 AsyncServerSocket.cpp:762] failed to set SO_REUSEPORT on async server socket Protocol not available
E0415 22:32:38.945001 15510 ThriftServer.cpp:440] Got an exception while setting up the server: 92failed to bind to async server socket: [::]:0: Protocol not available
E0415 22:32:38.945057 15510 RaftexService.cpp:90] Setup the Raftex Service failed, error: 92failed to bind to async server socket: [::]:0: Protocol not available
E0415 22:32:38.949586 15463 NebulaStore.cpp:47] Start the raft service failed
E0415 22:32:38.949597 15463 MetaDaemon.cpp:88] Nebula store init failed
E0415 22:32:38.949796 15463 MetaDaemon.cpp:215] Init kv failed!
```

此时服务状态为：

```bash
[root@redhat6 scripts]# ./nebula.service status  all
[WARN] The maximum files allowed to open might be too few: 1024
[INFO] nebula-metad: Exited
[INFO] nebula-graphd: Exited
[INFO] nebula-storaged: Running as 15547, Listening on 44500
```

出错原因：CentOS 6.5 系统内核版本为 2.6.32，`SO_REUSEPORT` 仅支持 `Linux 3.9` 及以上版本。

将系统升级到 CentOS 7.5 问题可自行解决。

### 同一个主机启动了两个 storaged，并使用了相同的配置和端口

不要这样使用，删除一个 storaged 的数据目录，重新导入数据。

### 存储 sst 文件是否支持拷贝给另一个图空间

storaged 所使用的 sst 文件和集群图空间是绑定的，不支持直接拷贝给一个新的图空间或者集群使用。
